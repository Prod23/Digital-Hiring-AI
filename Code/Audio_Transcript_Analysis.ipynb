{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yaH5Sih__an",
        "outputId": "0afaaaf6-6ac8-4641-d9e9-dba07ce995ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###We're using the audio and text files generated by the Whisper Library"
      ],
      "metadata": {
        "id": "omzcnuFJaoEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path='/content/drive/MyDrive/Techolution Final/one.wav'\n",
        "text_path='/content/drive/MyDrive/Techolution Final/one.txt'"
      ],
      "metadata": {
        "id": "ce2dRUG2AlZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Silence time Analysis from audio\n",
        "\n",
        "Librosa is an audio-processing library. We begin by loading the audio data and extracting the sample rate for further processing. A critical component of the code is setting a threshold value, denoted as 'threshold,' which acts as the basis for identifying silence in the audio. The threshold can be adjusted to suit the specific characteristics of the audio file. The code then iterates through the audio data, segmenting it into periods of silence and non-silence based on the amplitude's comparison to the threshold. It effectively detects transitions from non-silence to silence and vice versa.\n",
        "\n",
        "As the code progresses, it accumulates these silence periods within the 'silence_periods' list. Importantly, it calculates the duration of each detected silence period and prints it out if it exceeds a threshold of 3 seconds. Moreover, the code keeps a running total of the duration of prolonged silence periods in the 'count' variable. This information can be valuable for tasks such as audio transcription or speech analysis. Lastly, the code wraps up by determining the total length of the audio file in seconds, making use of Librosa's 'get_duration' function."
      ],
      "metadata": {
        "id": "ZpGrBoDYAYpA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw4P18lv_xJF",
        "outputId": "a33a0e8b-1ac8-4a7c-ef9a-98951e6b027a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silence Period 21: Duration 4.23 seconds\n",
            "Silence Period 32: Duration 3.06 seconds\n",
            "Silence Period 360: Duration 3.54 seconds\n",
            "Silence Period 398: Duration 3.99 seconds\n",
            "Silence Period 823: Duration 3.03 seconds\n",
            "Silence Period 848: Duration 3.97 seconds\n",
            "Silence Period 853: Duration 7.08 seconds\n",
            "Silence Period 870: Duration 4.42 seconds\n",
            "Silence Period 1415: Duration 6.62 seconds\n",
            "Silence Period 1658: Duration 3.38 seconds\n",
            "43.319319727891155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155.54"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "count = 0\n",
        "# Load the audio data and sample rate\n",
        "audio_path = '/content/drive/MyDrive/Techolution Final/one.wav'\n",
        "audio, sample_rate = librosa.load(audio_path)\n",
        "\n",
        "# Set a threshold for silence detection (adjust this value as needed)\n",
        "threshold = 0.4  # You may need to experiment with different values\n",
        "\n",
        "# Find silence periods\n",
        "silence_periods = []\n",
        "is_silent = True\n",
        "current_period = []\n",
        "for amplitude in audio:\n",
        "    if abs(amplitude) < threshold:\n",
        "        if not is_silent:\n",
        "            # Transition to silence\n",
        "            silence_periods.append(current_period)\n",
        "            current_period = []\n",
        "            is_silent = True\n",
        "        current_period.append(amplitude)\n",
        "    else:\n",
        "        if is_silent:\n",
        "            # Transition from silence\n",
        "            current_period.append(amplitude)\n",
        "            is_silent = False\n",
        "        else:\n",
        "            current_period.append(amplitude)\n",
        "\n",
        "# Append any remaining silent period\n",
        "if is_silent:\n",
        "    silence_periods.append(current_period)\n",
        "\n",
        "# Analyze or process the detected silence periods as needed\n",
        "for i, period in enumerate(silence_periods):\n",
        "    duration = len(period) / float(sample_rate)\n",
        "    if(duration > 3.00):\n",
        "      count+=duration\n",
        "      print(f\"Silence Period {i + 1}: Duration {duration:.2f} seconds\")\n",
        "print(count)\n",
        "audio_length_seconds = librosa.get_duration(y=audio, sr=sample_rate)\n",
        "audio_length_seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The below metric is used to quantify how confident an interviewee is, based on the silence that was observed during their interview. It's a simple ratio that uses the silence time divided by the total time.\n"
      ],
      "metadata": {
        "id": "jiFKI3bIbcTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "underconfidence_metric = count/audio_length_seconds\n",
        "underconfidence_metric\n",
        "\n",
        "confidence_metric = 1-underconfidence_metric\n",
        "confidence_metric"
      ],
      "metadata": {
        "id": "6E7lE9srYd5n",
        "outputId": "673cee95-cf14-4d3c-80c6-d375bb5a8927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7214908079729256"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Emotion Analysis from Transcript\n",
        "We read the text from the .txt file"
      ],
      "metadata": {
        "id": "41bvRgjEAS-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the text file you want to read\n",
        "file_path = \"/content/drive/MyDrive/Techolution Final/one.txt\"  # Replace with your file path\n",
        "\n",
        "# Open the file in read mode (default)\n",
        "with open(file_path, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Now, the file contents are stored in the 'file_contents' variable as a string\n",
        "\n",
        "# Print or use the string as needed\n",
        "print(transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2ZduwvPAQeW",
        "outputId": "838f4311-95ef-4e07-871c-2cae765d32c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So, my name is Dipanshu and I belong to ISA, which is in Haryana state and I've done my\n",
            "bachelor's from Vyamsi University of Science and Technology, which is in Paridabad.\n",
            "And so for the last 1.5 years and being the part of the ISQ organization as a Python developer.\n",
            "So during my tenure, I've been into many projects.\n",
            "So the main project I'm handling is the analytics.\n",
            "So in this project, I used to develop APIs for the admin users so that they can see\n",
            "the business insights of their business, business insights.\n",
            "So these API, these are the rest APIs, which are developed in the Django framework.\n",
            "And these API include the total sales, total orders.\n",
            "So how much the card payment, how much the online payment, so how much the cash payment.\n",
            "So these include all the stats for the business insights.\n",
            "And apart from that, I'm also handling one other clone, which is a fancy only fence clone.\n",
            "So in that project, I'm handling the whole admin panel and the posting service.\n",
            "So apart from that, I'm also handling the micro services also like notification and so.\n",
            "And I'm also handling many Chrome services and stream services for many projects.\n",
            "So the main database we are using here in Haryana by company is the Mongo database as\n",
            "primary.\n",
            "They also have experience with Cassandra as well.\n",
            "And I think that's all from my side.\n",
            "Okay, cool.\n",
            "So what is your normal text that look like?\n",
            "What?\n",
            "What is your normal text that look like?\n",
            "Text like?\n",
            "Look like.\n",
            "I don't know what the word you're saying.\n",
            "Okay.\n",
            "So what are the normal technologies or the normal technologies?\n",
            "So our technologies like I've used the Revit MQ, so Revit MQ in libraries and using pandas,\n",
            "mainly pandas, Mumbai, delivery, as we're discussing about the database.\n",
            "So the main primary database we're using is Mongo.\n",
            "And for the deployment, we are using dockerization, so I used to create the docker image for\n",
            "all my micro-service and code and the development is the dockerized version only.\n",
            "And yeah, so I'm also having experience with the AWS service.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###We perform Sentiment analysis using the VADER (Valence Aware Dictionary and sentiment Reasoner) sentiment analysis tool from the NLTK (Natural Language Toolkit) library."
      ],
      "metadata": {
        "id": "AIS_zn6AdRXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blGS2j9OAQcE",
        "outputId": "d5e2bb6f-7de8-4a12-d434-a2a154d3f67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VADER is a lexicon and rule-based sentiment analysis tool that assigns sentiment scores to text, helping determine whether the text expresses a positive, negative, or neutral sentiment.\n",
        "\n",
        "Following the initialization, the code analyzes the sentiment of a text or transcript, which should be stored in the 'transcript' variable. The sentiment analysis process computes various sentiment scores, including a 'compound' score, which represents an overall sentiment score for the text.\n",
        "\n",
        "The code then interprets the sentiment scores to assign a sentiment label. If the 'compound' score is greater than or equal to 0.05, it categorizes the sentiment as \"Positive.\" Conversely, if the 'compound' score is less than or equal to -0.05, it categorizes the sentiment as \"Negative.\" If the 'compound' score falls between these thresholds, the code labels the sentiment as \"Neutral.\"\n",
        "\n",
        "Lastly, the code prints out the sentiment analysis results, displaying both the sentiment scores, which provide a more granular view of positive, negative, and neutral sentiment, and the sentiment label, which simplifies the overall sentiment classification."
      ],
      "metadata": {
        "id": "iKXkOuvzdWJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Analyze sentiment\n",
        "sentiment_scores = analyzer.polarity_scores(transcript)\n",
        "\n",
        "# Interpret the sentiment scores\n",
        "sentiment_label = \"\"\n",
        "if sentiment_scores[\"compound\"] >= 0.05:\n",
        "    sentiment_label = \"Positive\"\n",
        "elif sentiment_scores[\"compound\"] <= -0.05:\n",
        "    sentiment_label = \"Negative\"\n",
        "else:\n",
        "    sentiment_label = \"Neutral\"\n",
        "\n",
        "# Print the sentiment analysis results\n",
        "print(\"Sentiment Scores:\", sentiment_scores)\n",
        "print(\"Sentiment Label:\", sentiment_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sxXkI2SAQZV",
        "outputId": "61a01e22-4b54-40bc-b5f9-e39a9eb7dd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.922, 'pos': 0.078, 'compound': 0.9742}\n",
            "Sentiment Label: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Number of Filler words\n",
        "The code takes the input text and converts it to lowercase to ensure case-insensitive matching. It then defines a list of common filler words such as ***\"the,\" \"and,\" \"in,\"*** and others. These filler words are often used in speech and writing but may not carry substantial meaning by themselves.\n",
        "\n",
        "Next, the code creates a regular expression pattern, 'filler_pattern,' that matches any of the defined filler words when they appear as whole words within the text. The 're.findall' function is used to find all instances of these filler words within the input text, taking into account case-insensitivity.\n",
        "\n",
        "The code counts the number of filler word occurrences and stores this count in the 'num_fillers' variable. Additionally, it retains the original text in the 'text' variable.\n",
        "\n",
        "After counting the filler words, the code proceeds to split the 'text' into individual words, using whitespace as a delimiter. This allows it to calculate the total word count, which it stores in the 'word_count' variable.\n",
        "\n",
        "Finally, the code prints out the number of filler words found in the text and the total word count."
      ],
      "metadata": {
        "id": "_sxglNcVWLxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def count_fillers(input_text):\n",
        "    # Convert all characters to lowercase\n",
        "    input_text = input_text.lower()\n",
        "\n",
        "    # Define a list of common filler words\n",
        "    filler_words = [\"the\", \"and\", \"in\", \"to\", \"of\", \"with\", \"on\", \"for\", \"a\", \"an\", \"so\"]\n",
        "\n",
        "    # Create a regular expression pattern to match filler words (case-insensitive)\n",
        "    filler_pattern = r'\\b(?:' + '|'.join(filler_words) + r')\\b'\n",
        "\n",
        "    # Use re.findall to find all occurrences of filler words\n",
        "    filler_matches = re.findall(filler_pattern, input_text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Count the number of filler words\n",
        "    num_fillers = len(filler_matches)\n",
        "\n",
        "    return num_fillers,input_text\n",
        "\n",
        "\n",
        "# Count the number of filler words in the text\n",
        "num_fillers,text = count_fillers(transcript)\n",
        "\n",
        "# Print the count\n",
        "print(\"Number of filler words:\", num_fillers)\n",
        "print(text)\n",
        "\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "# Count the number of words\n",
        "word_count = len(words)\n",
        "\n",
        "# Print the word count\n",
        "print(\"Number of words:\", word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwkrkqzzBpo6",
        "outputId": "7eafd690-e8da-4fdb-d4d0-e2bc6f50e7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of filler words: 92\n",
            "so, my name is dipanshu and i belong to isa, which is in haryana state and i've done my\n",
            "bachelor's from vyamsi university of science and technology, which is in paridabad.\n",
            "and so for the last 1.5 years and being the part of the isq organization as a python developer.\n",
            "so during my tenure, i've been into many projects.\n",
            "so the main project i'm handling is the analytics.\n",
            "so in this project, i used to develop apis for the admin users so that they can see\n",
            "the business insights of their business, business insights.\n",
            "so these api, these are the rest apis, which are developed in the django framework.\n",
            "and these api include the total sales, total orders.\n",
            "so how much the card payment, how much the online payment, so how much the cash payment.\n",
            "so these include all the stats for the business insights.\n",
            "and apart from that, i'm also handling one other clone, which is a fancy only fence clone.\n",
            "so in that project, i'm handling the whole admin panel and the posting service.\n",
            "so apart from that, i'm also handling the micro services also like notification and so.\n",
            "and i'm also handling many chrome services and stream services for many projects.\n",
            "so the main database we are using here in haryana by company is the mongo database as\n",
            "primary.\n",
            "they also have experience with cassandra as well.\n",
            "and i think that's all from my side.\n",
            "okay, cool.\n",
            "so what is your normal text that look like?\n",
            "what?\n",
            "what is your normal text that look like?\n",
            "text like?\n",
            "look like.\n",
            "i don't know what the word you're saying.\n",
            "okay.\n",
            "so what are the normal technologies or the normal technologies?\n",
            "so our technologies like i've used the revit mq, so revit mq in libraries and using pandas,\n",
            "mainly pandas, mumbai, delivery, as we're discussing about the database.\n",
            "so the main primary database we're using is mongo.\n",
            "and for the deployment, we are using dockerization, so i used to create the docker image for\n",
            "all my micro-service and code and the development is the dockerized version only.\n",
            "and yeah, so i'm also having experience with the aws service.\n",
            "\n",
            "Number of words: 358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The below code hels us find the percentage of words in the interview that were filler, thereby telling us how much useful the interview was in conveying the candidate's skillsets."
      ],
      "metadata": {
        "id": "WWAHT_hid4Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_filler_percentage=num_fillers/word_count\n",
        "metric_filler_percentage\n",
        "\n",
        "metric_canon_percentage = 1 - metric_filler_percentage\n",
        "metric_canon_percentage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P0atTnoWWQ7",
        "outputId": "2fbcad38-c6cb-4c5b-aa53-b28055034654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7430167597765363"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Metric_Communication = metric_canon_percentage*0.15\n",
        "Metric_Communication"
      ],
      "metadata": {
        "id": "7JvCj84DWzZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77b2bf2-d69f-451a-8168-66bafc8a56d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11145251396648044"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wu3JIUHQf9HS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}